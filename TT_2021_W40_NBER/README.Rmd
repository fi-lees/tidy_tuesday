---
title: "Tidy Tuesday: 2021, Week 40, NBER"
author: "Fiona Lees"
date: 2021-10-01
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      fig.width = 12, 
                      fig.asp = 0.618, 
                      out.width = "100%",
                      fig.align = "center")

```

## National Bureau of Economic Research
This week's data comes from the [National Bureau of Economic Research (NBER)](https://www.nber.org) and is provided via the [`nberwp` package by Ben Davies](https://github.com/bldavies/nberwp). The package contains data about NBER working papers published between June 1973 and July 2021. Information is provided about the papers, authorship and associated work programs.
  
### Objectives
I'm going to focus on the `papers` data frame. Each row describes a unique working paper (paper id, title, year / month of publication). I'm going to investigate which words are used most frequently in paper titles and whether this varies across the decades.
  
### Learning Points
This week I learned:  
- How to use the `tidytext` package to carry out some simple text mining. I used `unnest_tokens()` to split each title into individual words. [Text Mining with R](https://www.tidytextmining.com/index.html), by Julia Silge and David Robinson is a great introduction to text mining.  
- How to create word clouds using the `ggwordcloud` package. Not usually my data vis of choice, but it's good to know how to create them in R.
  
### Load Packages
Load the required packages.  

```{r packages}

library(ggwordcloud)
library(tidytext)
library(tidyverse)

```
  
### Import Data
Read in the `papers` data.

```{r import_data, results = FALSE}

papers <- readr::read_csv(
  "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/papers.csv"
  )

```
  
### Explore Data
#### Summary
Run a quick summary on `papers` to get an idea of what we have.  

```{r summary}

papers_new <- papers %>% 
  # Add a variable showing the decade of publication
  mutate(decade = factor(str_c(as.character(floor(year / 10) * 10),"s")), .before = year) %>% 
  # Use the first letter of each paper id to group papers into a catalogue series
  mutate(
    catalogue_series = str_sub(paper, 1, 1),
    catalogue_series = factor(case_when(
      catalogue_series == "h" ~ "Historical",
      catalogue_series == "t" ~ "Technical",
      catalogue_series == "w" ~ "General"
      )),
    .after = paper
  )

#skimr::skim(papers_new) # Sometimes I like to use skimr to summarise data 
summary(papers_new)

```
    
     
#### How many papers?
**Q:** How many papers were published each year?  
**A:** The number of papers increased from 45 in 1974 to 1,713 in 2020.   

```{r papers_per_year, fig.alt = "Bar chart showing the number of papers issued by the National Bureau of Economic Research each year between June 1973 and July 2021. The number of papers has increased from 45 in 1974 to 1,713 in 2020."}

# Paper counts for key years, to be used for labelling in the chart below
paper_counts_year <- papers_new %>% 
  count(year) %>% 
  rename(n_papers = n) %>% 
  filter(year %in% c(1974, 1980, 1990, 2000, 2010, 2020, 2021))

# Create a chart showing the number of papers published each year
papers_new %>% 
  ggplot(aes(x = year)) +
  geom_bar(fill = "#482878FF", width = 0.5) +
  geom_text(data = paper_counts_year, aes(y = n_papers, label = n_papers), nudge_y = 60, size = 3.5) +
  scale_x_continuous(limits = c(1970, 2025), breaks = seq(1970, 2025, 5)) +
  scale_y_continuous(limits = c(0, 1900), breaks = seq(0, 1900, 100)) +
  labs(title = "Number of papers published each year by the National Bureau of Economic Research",
       subtitle = "June 1973 - July 2021",
       x = "Year",
       y = "Number of papers")

```

  
#### How many title words?
**Q:** How many words / unique words were used in paper titles?   
**A:** After removing standard [stop words](https://en.wikipedia.org/wiki/Stop_word), numbers and words repeated in a single title: **words** = 175,096; **unique words** = 12,032 
  
```{r number_of_words}

# Split each paper title into individual words 
title_words <- papers_new %>%
  # Split title and format so that each word is on a separate row
  unnest_tokens(word, title, drop = FALSE) %>% 
  # Remove standard stop words (e.g. and, the, to, of)
  anti_join(stop_words) %>% 
  # Remove numbers (e.g. 1982, 19)
  filter(nchar(gsub('[a-z.]', '', word)) == 0) %>%
  # Remove duplicate words (those that appear in a title more than once)
  group_by(paper, word) %>% 
  mutate(word_instance = row_number()) %>% 
  filter(word_instance == 1) %>% 
  ungroup() %>%
  select(-catalogue_series, -month, -word_instance)

# How may words?
length(title_words$word)

# How many unique words?
n_distinct(title_words$word)

```

  
#### Most common title words
**Q:** What were the most common words used in paper titles across all years?   
**A:** (see list below). 

```{r most_common_words}

# Most common words?
title_words %>% 
  count(word, sort = TRUE) %>%
  rename(word_freq = n) %>% 
  print(n = 20)

```
  
  
#### Analysis specific stop words
There are some common words, in addition to classic stop words, that don't tell us much about the subject of a paper. For example, the word 'evidence' appears in a title around twice as often as any other word. I've created a list of these words and have chosen to filter them out for the rest of this analysis. It was quite hard to decide which words to add to this list and which to leave out - subjective decision making. I swithered over the words 'market' and 'markets', but I've left them in the analysis for the moment.

```{r other_stop_words}

# Define stop words specific to this analysis
other_stopwords <- c(
  "analysis",
  "effect",
  "effects",
  "estimation",
  "evidence",
  "model",
  "models",
  "policy",
  "rate",
  "rates",
  "theory",
  "u.s"
)

# Remove other stop words specific to this analysis
title_words_reduced <- title_words %>% 
  filter(!word %in% other_stopwords) 

# How may words now?
length(title_words_reduced$word)

# How many unique words now?
n_distinct(title_words_reduced$word)

```

  
#### Most common words, by decade
**Q:** What were the most common title words used in each decade?   
**A:** See summary below.

```{r summary_by_decade}
#  Create summary table showing frequency of word use in each decade.
summary_by_decade <- title_words_reduced %>% 
  group_by(decade) %>% 
  mutate(n_papers = n_distinct(paper)) %>% 
  group_by(decade, word) %>% 
  summarise(
    n_papers = min(n_papers), 
    word_freq_n = n(),
    word_freq_pct = (word_freq_n / n_papers) 
    ) %>% 
  mutate(decade_rank = dense_rank(desc(word_freq_n))) %>% 
  arrange(decade, decade_rank) %>% 
  ungroup()

# Show the top three words for each decade.
# Note that some words were used the same number of times and rank jointly with other words.
summary_by_decade %>% 
  filter(decade_rank <= 3) %>% 
  print(n = 20)

```


### Visualise Data
I'm going to explore different ways of visualising the top ten ranked words for each decade.
   
First, set the theme for the visualisations.

```{r set_theme}
theme_set(theme_minimal())

theme_update(text = element_text(colour = "grey40"),
             plot.title = element_text(colour = "black", size = 16, face = "bold", margin = margin(t = 5, b = 5)),
             plot.subtitle = element_text(size = 12, margin = margin(b = 10)),
             strip.text.x = element_text(size = 12, vjust = 1),
             axis.text = element_text(size = 11),
             axis.title.x = element_text(margin = margin(t = 10, b = 10), hjust = 0.0),
             panel.grid.major.y = element_blank(),
             panel.grid.minor = element_blank(),
             plot.margin = margin(rep(8, 4)),
             legend.position = "none"
             )

```

   
#### Chart 1: Ten most common title words, by decade (number of times used)
A helpful first draft, but this chart doesn't account for the increasing number of papers published each decade. The bars are also quite 'heavy'. Note that some words were used the same number of times and rank jointly with other words, so there may be more than ten words listed for each decade. I could have used `slice_head()`rather than `filter()` to keep to ten words, but I chose not to. 

```{r chart_1, fig.asp = 0.80, fig.alt = "Series of bar charts showing the ten most common words used in the titles of NBER papers in each decade (1970s - 2020s). Data available from June 1973 until July 2021 and presented as the number of times a word was used in a title. Many words are the same in each decade (market, capital), but some new ones have appeared (risk)."}

# Create new labels for the facet strips
decade_labels <- c("1970s" = "1970s\n(June 1973 +)", 
                   "1980s" = "1980s", 
                   "1990s" = "1990s", 
                   "2000s" = "2000s", 
                   "2010s" = "2010s", 
                   "2020s" =  "2020s\n(until July 2021)")

# Create chart
chart_1 <- summary_by_decade %>% 
  filter(decade_rank <= 10) %>% 
  # Reorder the words by frequency within each decade so that they are shown in order
  mutate(word = reorder_within(word, word_freq_n, decade)) %>% 
  ggplot(aes(x = word_freq_n, y = word)) +
  geom_col(fill = "#482878FF", width = 0.5) +
  scale_y_reordered() +
  facet_wrap(~ decade, scales = "free_y", ncol = 3, labeller = labeller(decade = decade_labels)) +
  labs(title = "Ten most common words used in the titles of NBER papers, by decade",
       subtitle = "Some words rank equally, therefore there may be more than ten words listed for each decade",
       x = "Number of times word was used in a title",
       y = "",
       caption = "Tidy Tuesday: Week 40, 2021 | Data source: NBER (via the nberwp package) | Visualisation: @Fi_Lees")

chart_1

```

   
#### Chart 2: Ten most common title words, by decade (percentage of times used)
This chart accounts for the increasing number of papers each decade by showing the percentage of titles in which a word was used. I've changed it to a lollipop chart to make it less 'heavy'. I've also colour coded the points according to rank (1-10) just to see how this looks.

```{r chart_2, fig.asp = 0.80, fig.alt = "Series of lollipop charts showing the ten most common words used in the titles of NBER papers in each decade (1970s - 2020s). Data available from June 1973 until July 2021 and presented as the percentage of titles in which a word was used. Many words are the same in each decade (market, capital), but some new ones have appeared (risk)."}

chart_2 <- summary_by_decade %>% 
  filter(decade_rank <= 10) %>% 
  mutate(word = reorder_within(word, word_freq_pct, decade)) %>% 
  ggplot() +
  # Lollipop stick
  geom_segment(aes(x = 0, xend = word_freq_pct, y = word, yend = word), colour = "grey") +
  # Lollipop head
  geom_point(aes(x = word_freq_pct, y = word, colour = decade_rank), size = 3.5) +
  scale_colour_viridis_b(direction = 1, option = "D", end = 0.8) +
  scale_y_reordered() +
  scale_x_continuous(
    labels = scales::percent_format(accuracy = 1),
    limits = c(0, 0.12), 
    breaks = seq(0, 0.12, 0.02)
    ) +
  facet_wrap(~ decade, scales = "free_y", ncol = 3, labeller = labeller(decade = decade_labels)) +
  labs(title = "Ten most common words used in the titles of NBER papers, by decade",
       subtitle = "Some words rank equally, therefore there may be more than ten words listed for each decade",
       x = "Percentage of titles in which word was used",
       y = "",
       caption = "Tidy Tuesday: Week 40, 2021 | Data source: NBER (via the nberwp package) | Visualisation: @Fi_Lees")

chart_2

```

   
#### Chart 3: Ten most common title words, by decade (word cloud)
This shows the same information as Chart 2, but in the form of a word cloud. Word clouds don't provide precise information, but they can give you a quick idea of word frequency. Can be useful when precision isn't the goal.  

There are various parameters that can be tweaked to change the look of a word cloud; this [`ggwordcloud` vignette](https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html) gives a really helpful overview.

```{r chart_3, fig.asp = 0.55, fig.alt = "Series of word clouds showing the ten most common words used in the titles of NBER papers in each decade (1970s - 2020s). Data available from June 1973 until July 2021. Many words are the same in each decade (market, capital), but some new ones have appeared (risk)."}

# Create different labels for the facet strips (no line breaks this time)
decade_labels_2 <- c("1970s" = "1970s (June 1973 +)", 
                     "1980s" = "1980s", 
                     "1990s" = "1990s", 
                     "2000s" = "2000s", 
                     "2010s" = "2010s", 
                     "2020s" =  "2020s (until July 2021)")

# Create word cloud
set.seed(42)
chart_3 <- summary_by_decade %>% 
  filter(decade_rank <= 10) %>%
  ggplot(aes(label = word, size = word_freq_pct, colour = decade_rank)) +
  # Draw word cloud and set eccentricity of the spiral
  geom_text_wordcloud_area(eccentricity = 1) +
  # Set size of word cloud text area
  scale_size_area(max_size = 15) +
  scale_colour_viridis_b(direction = 1, option = "D", end = 0.8) +
  facet_wrap(~ decade, ncol = 3, labeller = labeller(decade = decade_labels_2)) +
  theme(plot.subtitle = element_text(margin = margin(b = 20)),
        strip.text.x = element_text(size = 14, face = "bold", vjust = 1)) +
  labs(title = "Ten most common words used in the titles of NBER papers, by decade",
       subtitle = "Some words rank equally, therefore there may be more than ten words listed for each decade",
       caption = "Tidy Tuesday: Week 40, 2021 | Data source: NBER (via the nberwp package) | Visualisation: @Fi_Lees")

chart_3

```

Save this visualisation:

```{r save}

ggsave("wordcloud.png", chart_3 , width = 12, height = 8, units = "in", dpi = 300)

```

   
#### Chart 4: Ten most common title words, by decade (first time in top ten)
Similar to Chart 2, but highlighting new entries to the top ten each decade. Many top ten words are the same in each decade. For the 2020s we need to remember we're only a year-and-a-half into this decade; the top ten word list for the 2020s might look very different by 2029.

```{r chart_4, fig.asp = 0.85, fig.alt = "Series of lollipop charts showing the ten most common words used in the titles of NBER papers in each decade (1970s - 2020s). Data available from June 1973 until July 2021 and presented as the percentage of titles in which a word was used. New entries to the top ten each decade are highlighted. Many words are the same in each decade (market, capital), but some new ones have appeared (risk)."}

# Flag words that are a new entry to the top ten (never appeared in any previous decade)
top_ten_decade <- summary_by_decade %>% 
  filter(decade_rank <= 10) %>% 
  group_by(word) %>% 
  mutate(word_instance = row_number(),
         new_entry = factor(ifelse(word_instance == 1, "Yes", "No")),
         new_entry = factor(new_entry, levels = c("Yes", "No"))
         ) %>% 
  ungroup()

chart_4 <- top_ten_decade %>% 
  mutate(word = reorder_within(word, word_freq_pct, decade)) %>% 
  ggplot() +
  geom_segment(aes(x = 0, xend = word_freq_pct, y = word, yend = word), colour = "grey") +
  geom_point(aes(x = word_freq_pct, y = word, colour = new_entry), size = 3) +
  scale_colour_manual(values = c("#482878FF", "grey")) +
  scale_y_reordered() +
  scale_x_continuous(
    labels = scales::percent_format(accuracy = 1),
    limits = c(0, 0.12), 
    breaks = seq(0, 0.12, 0.02)
    ) +
  facet_wrap(~ decade, scales = "free_y", ncol = 3, labeller = labeller(decade = decade_labels)) +
  theme(
    legend.position = "top", 
    legend.justification = "left", 
    legend.title=element_text(size = 12),
    legend.text=element_text(size = 12)) +
  labs(title = "Ten most common words used in the titles of NBER papers, by decade",
       subtitle = "Some words rank equally, therefore there may be more than ten words listed for each decade",
       x = "Percentage of titles in which word was used",
       y = "",
       colour = "First time in top ten words list? ",
       caption = "Tidy Tuesday: Week 40, 2021 | Data source: NBER (via the nberwp package) | Visualisation: @Fi_Lees")

chart_4

```

  
### Final Notes
This is a very high level analysis and there are lots of other ways I could have sliced the data and presented it. This is the first time I've tried any text based analysis in R, but I'm looking forward to trying out more sophisticated projects using the `tidytext` package.


### Session Information
```{r session_info}

sessionInfo()

```


